# Improving Deep Neural Networks Hyperparameter tuning, Regularization and Optimization

**Week 1**

- Train/ Dev/ Test sets, Bias/ variance, Regularization, Forbenius norm, Weight decay,  L1 and L2 regularization, Dropout, Data augmentation, Early stopping, Orthogonalization, Vanishing and Exploding Gradients, Numerical approximation of Gradients, Gradient checking

**Week 2**

- Mini batch gradient descent, Exponentially weighted Moving Averages, stochastic gradient descent, Bias correction, Gradient descent with momentum, RMSprop, Adam optimization algorithm, Learning rate decay, Loca optima

**Week 3**

- Tuning process, Using and appropiate scale to pick hyperparameters, Normalizing activations, Batch norm
